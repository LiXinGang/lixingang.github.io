
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Xiu-Shen WEI （魏秀参）</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">

<td id="layout-content">
<div id="toptitle">
<h1>Xiu-Shen WEI （魏秀参）</h1>
</div>
<table class="imgtable"><tr><td>
<img src="weixs.jpeg" alt="weixs" width="175px" height="230px" />&nbsp;</td>
<td align="left"><p><a href="http://www.weixiushen.com/">Xiu-Shen WEI</a> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://megvii.com"><img src="megvii.png" alt=“megvii” width="170px" height="40px" /></a><br />
Ph.D., Research Director<br />
<a href="https://megvii.com/">Megvii</a> Research Nanjing (旷视南京研究院)<br /><br /></p>
<p>Office: 15th Floor, Tower A, No. 6 Xingzhi Rd., Nanjing<br />
Email: weixs.gm<img class="eq" src="eqs/8192024641-130.png" alt="@" style="vertical-align: -1px" />gmail.com <br /><br /></p>
<p>Media: <a href="https://scholar.google.com/citations?user=Qzyy5mcAAAAJ&hl=en"><img src="GoogleScholar.png" alt=“GoogleScholar” width="20px" height="20px" /></a> <a href="https://www.linkedin.com/in/xiu-shen-wei-%E9%AD%8F%E7%A7%80%E5%8F%82-11488086/"><img src="linkedin.png" alt=“linkedin” width="20px" height="20px" /></a> <a href="http://weibo.com/u/2618378195"><img src="weibo.png" alt="weibo" width="20px" height="20px" /></a> <a href="http://www.zhihu.com/people/wei-xiu-shen"><img src="zhihu.png" alt="zhihu" width="20px" height="20px" /></a></p>
</td></tr></table>
<h2>About Me</h2>
<ul>
<li><p>I am currently the Research Director of Megvii Research Nanjing, <a href="https://megvii.com/">Megvii Technology</a>. We focus on developing novel computer vision systems, creating new deep learning models, publishing high-quality papers and deploying cutting-edge technologies to better serve Megvii's business.</p>
</li>
</ul>
<ul>
<li><p>I got my Ph.D. in <a href="http://www.lamda.nju.edu.cn/CH.MainPage.ashx">LAMDA Group</a> at <a href="http://www.nju.edu.cn/">Nanjing University</a>, advised by Prof. <a href="http://cs.nju.edu.cn/wujx/">Jianxin Wu</a>. Before that, I received my B.Sc. degree in Computer Science and Technology in 2012, and got my M.Sc. degree in 2014 under the supervision of Prof. <a href="http://cs.nju.edu.cn/zhouzh/">Zhi-Hua Zhou</a>.</p>
</li>
</ul>
<ul>
<li><p>From Oct. 2016 to Oct. 2017, I was a visiting scholar in Prof. <a href="http://cs.adelaide.edu.au/~chhshen/">Chunhua Shen</a>’s group at <a href="https://www.adelaide.edu.au/">The University of Adelaide</a>, Australia.</p>
</li>
</ul>
<h2>Updates</h2>
<ul>
<li><p><p style="color:red"><b>Apr. 28, 2020: We release a PyTorch-based library for unsupervised image retrieval by deep CNNs on <a href="https://github.com/PyRetri/PyRetri">GitHub</a>! </b></p></p>
</li>
<li><p><p style="color:red"><b>Apr. 21, 2020: Our work (<a href="publication/tip17SCDA.pdf">SCDA</a>) about fine-grained image retrieval was selected as the highly cited paper by ESI!</b></p></p>
</li>
<li><p>Feb. 27, 2020: Two papers accepted to <a href="http://cvpr2020.thecvf.com/">CVPR 2020</a>: one for long-tailed recognition and one for domain adaptive object detection.</p>
</li>
<li><p>Feb. 20, 2020: Co-organized a workshop on <a href="http://www.robustvision.net/"><i>Robust Vision Challenge</i></a> at <a href="https://eccv2020.eu/">ECCV 2020</a>.
</p>
</li>
<li><p>Jul. 8, 2019: Organized a tutorial on <a href="http://www.icme2019.org/conf_tutorials"><i>Fine-Grained Image Analysis</i></a> at <a href="http://www.icme2019.org/">ICME 2019</a>.</p>
</li>
<li><p>Jul. 6, 2019: Opened the &ldquo;<a href="http://www.weixiushen.com/project/Awesome_FGIA/Awesome_FGIA.html">Awesome Fine-Grained Image Analysis – Papers, Codes and Datasets</a>&rdquo; homepage.
</p>
</li>
<li><p>Jun. 12, 2019: Our team won the <b>first place</b> in <b>both <a href="https://www.kaggle.com/c/inaturalist-2019-fgvc6/leaderboard">iNaturalist</a> and <a href="https://www.kaggle.com/c/herbarium-2019-fgvc6/leaderboard">Herbarium</a> tracks</b> of FGVC 2019 global competition!



</p>
</li>
<li><p>Nov. 1, 2018: <b>Published <a href="https://detail.tmall.com/item.htm?spm=a230r.1.14.13.443951e1EzWpSc&amp;id=581180454111&amp;ns=1&amp;abbucket=6">a Chinese book on convolutional neural networks and computer vision</a></b>.




</p>
</li>
</ul>
<h2>Research Interests</h2>
<p>My research interests include some sub-fields of <b>Computer Vision</b> and <b>Machine Learning</b>:</p>
<ul>
<li><p><b>Deep Convolutional Neural Networks</b> (DCNN) is a type of feed-forward artificial neural network where the individual neurons are tiled in such a way that they respond to overlapping regions in the visual field, which is widely used in image and video related tasks.</p>
</li>
</ul>
<ul>
<li><p><b>Fine-Grained Image Analysis</b> (FGIA) is a hot topic in computer vision and pattern recognition. The goals of FGIA are localizing the fine-grained objects, recognizing the objects&rsquo; subordinate categories, retrieving the fine-grained objects and so on.</p>
</li>
</ul>
<ul>
<li><p><b>Long-Tailed Distribution Learning</b> (LTDL) deals with real-world datasets displaying skewed distributions with <i>a long tail</i>, i.e., a few classes (a.k.a. head classes) occupy most of the data, while most classes (a.k.a. tail classes) have rarely few samples.</p>
</li>
</ul>
<ul>
<li><p><b>General Object Detection</b> is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class in digital images and videos.</p>
</li>
</ul>
<ul>
<li><p><b>Weakly Supervised Learning</b> (WSL), especially Multi-Instance Learning (MIL), is a variation on supervised learning. Instead of receiving a set of instances which are individually labeled, the learner receives a set of labeled <i>bags</i>, each containing many instances.</p>
</li>
</ul>
<ul>
<li><p><b>Bag-of-Words Model</b> (BoW) can be applied to image related tasks by encoding local visual descriptors of one image into a high dimensional vector. In BoW, there also include <i>Vector of the Locally Aggregated Descriptors</i> (<i>VLAD</i>) and <i>Fisher Vector</i> (<i>FV</i>).</p>
</li>
</ul>
<h2>Selected Publications</h2>
<p><tt>My papers are available for download in this <a href="./papers.html">Publications</a> page, and here is my  <a href="https://scholar.google.com/citations?user=Qzyy5mcAAAAJ&amp;hl=en">Google Scholar Citations profile</a>.</tt>
<br /><br /></p>
<ul>
<li><p><a href="publication/tip17SCDA.pdf">Selective Convolutional Descriptor Aggregation for Fine-Grained Image Retrieval</a>.
<br /><p style="color:red"><i>(This work was the highly cited paper by ESI, which received enough citations to place it in the top 1% of its academic field.)</i></p>
<b>X.-S. Wei</b>, J.-H. Luo, J. Wu, and Z.-H. Zhou.
<br /><i>IEEE Transactions on Image Processing (TIP)</i>, 2017, 26(6): 2868-2881.</p>
</li>
</ul>
<ul>
<li><p><a href="publication/cvpr20_BBN.pdf">BBN: Bilateral-Branch Network with Cumulative Learning for Long-Tailed Visual Recognition</a>. (<b><i>Oral presentation</i></b>)
<br /><p style="color:red"><i>(This work was the winner solution of the iNaturalist 2019 global competition.)</i></p>

B. Zhou, Q. Cui, <b>X.-S. Wei*</b>, and Z.-M. Chen.
<br /><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR’20)</i>, Seattle, WA, 2020, pp. xx-xx.</p>
</li>
</ul>
<ul>
<li><p><a href="publication/tip19PCM.pdf">Piecewise Classifier Mappings: Learning Fine-Grained Learners for Novel Categories with Few Examples</a>.
<br /><b>X.-S. Wei</b>, P. Wang, L. Liu, C. Shen, and J. Wu.
<br /><i>IEEE Transactions on Image Processing (TIP)</i>, 2019, 28(12): 6116-6125.</p>
</li>
</ul>
<ul>
<li><p><a href="publication/cvpr20_CRDADET.pdf">Exploring Categorical Regularization for Domain Adaptive Object Detection</a>.
<br />C. Xu, X. Zhao, X. Jin, and <b>X.-S. Wei*</b>.
<br /><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR’20)</i>, Seattle, WA, 2020, pp. xx-xx.</p>
</li>
</ul>
<ul>
<li><p><a href="publication/cvpr19.pdf">Multi-Label Image Recognition with Graph Convolutional Networks</a>.
<br />Z.-M. Chen, <b>X.-S. Wei*</b>, Peng Wang, and Y. Guo.
<br /><i>IEEE Conference on Computer Vision and Pattern Recognition (CVPR’19)</i>, Long Beach, CA, 2019, pp. 5177-5186.</p>
</li>
</ul>
<ul>
<li><p><a href="publication/mlj16.pdf">An Empirical Study on Image Bag Generators for Multi-Instance Learning</a>.
<br /><b>X.-S. Wei</b>, and Z.-H. Zhou.
<br /><i>Machine Learning</i>, 2016, 105(2): 155-198.</p>
</li>
</ul>
<ul>
<li><p><a href="publication/pr17.pdf">Mask-CNN: Localizing Parts and Selecting Descriptors for Fine-Grained Bird Species Categorization</a>.
<br /><b>X.-S. Wei</b>, C.-W. Xie, J. Wu, and C. Shen.
<br /><i>Pattern Recognition</i>, 2018, 76:704-714.</p>
</li>
</ul>
<h2>Professional Activities</h2>
<h3>Journal Reviewer (Selected)</h3>
<ul>
<li><p>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</p>
</li>
<li><p>IEEE Transactions on Image Processing (TIP)</p>
</li>
<li><p>Springer Journal of Machine Learning (MLJ)</p>
</li>
<li><p>IEEE Transactions on Neural Networks and Learning Systems (TNNLS)</p>
</li>
<li><p>IEEE Transactions on Multimedia (TMM)</p>
</li>
<li><p>Elsevier Journal of Neural Networks (NN)</p>
</li>
<li><p>Elsevier Journal of Pattern Recognition (PR)
</p>
</li>
<li><p>SCIENCE CHINA Information Sciences</p>
</li>
<li><p>…



</p>
</li>
</ul>
<h3>Conference Reviewer / Program Committee Member (Selected)</h3>
<ul>
<li><p>CVPR <a href="http://cvpr2017.thecvf.com/">2017</a>, <a href="http://cvpr2018.thecvf.com/">2018</a>, <a href="http://cvpr2019.thecvf.com/">2019</a>, <a href="http://cvpr2020.thecvf.com/">2020</a>, ICCV <a href="http://iccv2017.thecvf.com/">2017</a>,  <a href="http://iccv2019.thecvf.com/">2019</a>, ECCV <a href="https://eccv2018.org/">2018</a>, NIPS <a href="https://nips.cc/Conferences/2016/">2016</a>, <a href="https://nips.cc/Conferences/2018/">2018</a>, IJCAI <a href="https://www.ijcai-18.org/">2018</a>, <a href="https://ijcai19.org/">2019</a>,  <a href="https://www.ijcai20.org/">2020</a>, AAAI <a href="https://www.aaai.org/Conferences/AAAI/aaai16.php">2016</a>, <a href="https://www.aaai.org/Conferences/AAAI/aaai17.php">2017</a>, <a href="https://aaai.org/Conferences/AAAI-18/">2018</a>, <a href="https://aaai.org/Conferences/AAAI-19/">2019</a>, <a href="http://www.wikicfp.com/cfp/servlet/event.showcfp?eventid=90881">2020</a>, ACCV <a href="http://accv2018.net/">2018</a>, BMVC <a href="https://bmvc2019.org/">2019</a>, PRCV <a href="http://www.prcv2019.com/#/">2019</a>.
</p>
</li>
</ul>
<h2>Correspondence</h2>
<h3>Mail</h3>
<p>Xiu-Shen WEI <br />
Megvii Research Nanjing <br />
Nanjing 210000, China</p>
<h3>Office</h3>
<p>15th Floor, Tower A, No. 6 Xingzhi Rd., Nanjing
<br /><br /></p>
<p>(本网页ICP证：<a href="http://www.beian.miit.gov.cn/">苏ICP备19033388号</a>)</p>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=edf7b1&w=280&d=ZMgK85h8i8CPsDesJOcErsg6hWfvO0U12TfPKpRCwNk&t=tt&ct=ffffff&co=2c81cf'></script>
<div id="footer">
<div id="footer-text">
Page generated 2020-05-09 15:06:01 CST, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
